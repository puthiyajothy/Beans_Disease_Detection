{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = 256\n",
    "CHANNELS = 3\n",
    "EPOCHS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset using image_dataset_from_directory\n",
    "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"beansLeaf\",\n",
    "    seed=123,\n",
    "    shuffle=True,\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "class_names = dataset.class_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, and test sets\n",
    "test_ds = dataset.map(lambda x, y: (x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to NumPy arrays\n",
    "images = []\n",
    "labels = []\n",
    "for image_batch, label_batch in dataset:\n",
    "    images.append(image_batch.numpy())\n",
    "    labels.append(label_batch.numpy())\n",
    "images = np.concatenate(images)\n",
    "labels = np.concatenate(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the ImageDataGenerator with desired augmentation options\n",
    "data_augmentation = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    vertical_flip=False,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory if it doesn't exist\n",
    "augmented_images_dir = \"augmented_images\"\n",
    "if not os.path.exists(augmented_images_dir):\n",
    "    os.makedirs(augmented_images_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate augmented images and save them to the directory\n",
    "augmented_data_generator = data_augmentation.flow(images, labels, batch_size=BATCH_SIZE, shuffle=True)\n",
    "for i, (augmented_images, augmented_labels) in enumerate(augmented_data_generator):\n",
    "    for j, augmented_image in enumerate(augmented_images):\n",
    "        save_path = os.path.join(augmented_images_dir, f\"augmented_image_{i * BATCH_SIZE + j}.png\")\n",
    "        tf.keras.preprocessing.image.save_img(save_path, augmented_image)\n",
    "    if (i + 1) * BATCH_SIZE >= len(images):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a generator function for augmented image batches and labels\n",
    "def augmented_data_generator():\n",
    "    for image_batch, label_batch in data_augmentation.flow(images, labels, batch_size=BATCH_SIZE, shuffle=False):\n",
    "        yield image_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataset from the generator function\n",
    "augmented_dataset = tf.data.Dataset.from_generator(\n",
    "    augmented_data_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(BATCH_SIZE,), dtype=tf.int32)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the augmented dataset and custom dataset\n",
    "combined_dataset = augmented_dataset.concatenate(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Shuffle and batch the combined dataset\n",
    "combined_dataset = combined_dataset.shuffle(len(images)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the combined dataset into training and validation sets\n",
    "val_size = int(0.2 * len(images))\n",
    "train_dataset = combined_dataset.skip(val_size)\n",
    "val_dataset = combined_dataset.take(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create the model with the given hyperparameters\n",
    "def create_model(lr, batch_size, n_layers):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.experimental.preprocessing.Rescaling(1./255, input_shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS)))\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    for _ in range(n_layers):\n",
    "        model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(len(class_names), activation='softmax'))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=lr),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "batch_sizes = [16, 32, 64]\n",
    "num_layers = [2, 3, 4]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        for n_layers in num_layers:\n",
    "            model = create_model(lr, batch_size, n_layers)\n",
    "            history = model.fit(train_dataset, validation_data=val_dataset, epochs=EPOCHS)\n",
    "            accuracy = history.history['val_accuracy'][-1]\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model on the test set\n",
    "test_loss, test_accuracy = best_model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Save the best model\n",
    "best_model.save(\"../beansmodelhyper.h5\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
