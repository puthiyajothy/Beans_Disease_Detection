{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 256\n",
    "batch_size = 32\n",
    "CHANNELS = 3\n",
    "EPOCHS = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset using image_dataset_from_directory\n",
    "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"beansLeaf\",\n",
    "    seed=123,\n",
    "    shuffle=True,\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "class_names = dataset.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert dataset to NumPy arrays\n",
    "images = []\n",
    "labels = []\n",
    "for image_batch, label_batch in dataset:\n",
    "    images.append(image_batch.numpy())\n",
    "    labels.append(label_batch.numpy())\n",
    "images = np.concatenate(images)\n",
    "labels = np.concatenate(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the ImageDataGenerator with desired augmentation options\n",
    "data_augmentation = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    vertical_flip=False,\n",
    "    fill_mode='nearest'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the directory if it doesn't exist\n",
    "augmented_images_dir = \"augmented_images\"\n",
    "if not os.path.exists(augmented_images_dir):\n",
    "    os.makedirs(augmented_images_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate augmented images and save them to the directory\n",
    "augmented_data_generator = data_augmentation.flow(images, labels, batch_size=batch_size, shuffle=True)\n",
    "for i, (augmented_images, augmented_labels) in enumerate(augmented_data_generator):\n",
    "    for j, augmented_image in enumerate(augmented_images):\n",
    "        save_path = os.path.join(augmented_images_dir, f\"augmented_image_{i * batch_size + j}.png\")\n",
    "        tf.keras.preprocessing.image.save_img(save_path, augmented_image)\n",
    "    if (i + 1) * batch_size >= len(images):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a generator function for augmented image batches and labels\n",
    "def augmented_data_generator():\n",
    "    for image_batch, label_batch in data_augmentation.flow(images, labels, batch_size=batch_size, shuffle=False):\n",
    "        yield image_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a new dataset from the generator function\n",
    "augmented_dataset = tf.data.Dataset.from_generator(\n",
    "    augmented_data_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(batch_size, IMAGE_SIZE, IMAGE_SIZE, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(batch_size,), dtype=tf.int32)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the original dataset with the augmented dataset\n",
    "combined_dataset = dataset.concatenate(augmented_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the combined dataset into training and validation sets\n",
    "val_size = int(0.2 * len(images))\n",
    "train_dataset = combined_dataset.skip(val_size)\n",
    "val_dataset = combined_dataset.take(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Layer for Resizing and Normalization\n",
    "resize_and_rescale = tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
    "  layers.experimental.preprocessing.Rescaling(1./255),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "  layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Applying Data Augmentation to Train Dataset\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda x, y: (data_augmentation(x, training=True), y)\n",
    ").prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "input_shape = (IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n",
    "n_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained RNN model\n",
    "rnn_model = keras.applications.MobileNetV2(\n",
    "    input_shape=input_shape,\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Freeze the layers of the pre-trained model\n",
    "for layer in rnn_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CRNN model by adding RNN on top of the pre-trained model\n",
    "model = models.Sequential([\n",
    "    resize_and_rescale,\n",
    "    rnn_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(n_classes, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CRNN model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=val_dataset,\n",
    "    verbose=1,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the CRNN model on the test dataset\n",
    "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"beansLeaf/test\",\n",
    "    seed=123,\n",
    "    shuffle=False,\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(test_dataset)\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the evaluation results\n",
    "print(\"Test accuracy:\", scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation accuracy\n",
    "plt.plot(acc)\n",
    "plt.plot(val_acc)\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss\n",
    "plt.plot(loss)\n",
    "plt.plot(val_loss)\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on a sample image\n",
    "test_images = []\n",
    "test_labels = []\n",
    "for image_batch, label_batch in test_dataset:\n",
    "    test_images.append(image_batch.numpy())\n",
    "    test_labels.append(label_batch.numpy())\n",
    "test_images = np.concatenate(test_images)\n",
    "test_labels = np.concatenate(test_labels)\n",
    "\n",
    "for i in range(9):\n",
    "    test_image = test_images[i]\n",
    "    test_label = test_labels[i]\n",
    "    predicted_class = class_names[np.argmax(model.predict(np.expand_dims(test_image, axis=0)))]\n",
    "    actual_class = class_names[test_label]\n",
    "    \n",
    "    plt.imshow(test_image.astype(\"uint8\"))\n",
    "    plt.title(f\"Actual: {actual_class}, Predicted: {predicted_class}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
